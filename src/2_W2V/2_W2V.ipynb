{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NLP CARES - Model Insf. Aórtica\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "**Librarys**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The watermark extension is already loaded. To reload it, use:\n",
      "  %reload_ext watermark\n",
      "gensim           3.8.1\n",
      "spacy            2.2.3\n",
      "matplotlib.pylab 1.17.2\n",
      "re               2.2.1\n",
      "pandas           0.25.1\n",
      "logging          0.5.1.2\n",
      "numpy            1.17.2\n",
      "matplotlib       3.1.1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%load_ext watermark\n",
    "%watermark --iversions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np\n",
    "from numba import jit\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "#NLP\n",
    "from spacy.lang.es.stop_words import STOP_WORDS\n",
    "import spacy\n",
    "#The vocabulary in spanish\n",
    "nlp = spacy.load('es_core_news_sm')\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "%pylab inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "**Import Dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_hdf('../../data/data.h5', 'TRAIN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df_train.drop([i for i in df_train.index if not df_train.loc[i, 'tokenized_sentences']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Classification model\n",
    "\n",
    "Create two df, Create two dataframes, one with the column \"Insf_Mv\" not null values, and the other with the null values in this column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp.vocab[\"no\"].is_stop = False\n",
    "nlp.vocab[\"realizado\"].is_stop = False\n",
    "nlp.vocab[\"sin\"].is_stop = False\n",
    "nlp.vocab[\"tener\"].is_stop = False\n",
    "nlp.vocab[\"manifestó\"].is_stop = False\n",
    "nlp.vocab[\"existe\"].is_stop = False\n",
    "nlp.vocab[\"considera\"].is_stop = False\n",
    "nlp.vocab[\"estados\"].is_stop = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a list with the column \"CONCLUSION_CLEAN\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = []\n",
    "for sentence_group in df_train['tokenized_sentences']:\n",
    "    sentences.extend(sentence_group)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 29.6 s, sys: 0 ns, total: 29.6 s\n",
      "Wall time: 21.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Set values for various parameters\n",
    "num_features = 200    # Word vector dimensionality\n",
    "min_word_count = 3    # Minimum word count\n",
    "num_workers = 4       # Number of threads to run in parallel\n",
    "context = 8           # Context window size\n",
    "downsampling = 1e-3   # Downsample setting for frequent words\n",
    "\n",
    "\n",
    "# Initialize and train the model\n",
    "W2Vmodel = Word2Vec(sentences=sentences,\n",
    "                    sg=0,\n",
    "                    hs=1,\n",
    "                    workers=num_workers,\n",
    "                    size=num_features,\n",
    "                    min_count=min_word_count,\n",
    "                    window=context,\n",
    "                    sample=downsampling,\n",
    "                    negative=5,\n",
    "                    iter=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(W2Vmodel, open(\"../Final_Models/W2Vmodel.pickle\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_nlp)",
   "language": "python",
   "name": "conda_nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
