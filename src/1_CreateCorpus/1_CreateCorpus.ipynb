{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NLP CARES - Create corpus\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "**Librerías**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext watermark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "numpy  1.17.2\n",
      "re     2.2.1\n",
      "spacy  2.2.3\n",
      "pandas 0.25.1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%watermark --iversions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "#Stop words and the vocabulary in spanish\n",
    "from spacy.lang.es.stop_words import STOP_WORDS\n",
    "import spacy\n",
    "#The vocabulary in spanish\n",
    "nlp = spacy.load('es_core_news_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp.vocab[\"no\"].is_stop = False\n",
    "nlp.vocab[\"realizado\"].is_stop = False\n",
    "nlp.vocab[\"sin\"].is_stop = False\n",
    "nlp.vocab[\"tener\"].is_stop = False\n",
    "nlp.vocab[\"manifestó\"].is_stop = False\n",
    "nlp.vocab[\"existe\"].is_stop = False\n",
    "nlp.vocab[\"considera\"].is_stop = False\n",
    "nlp.vocab[\"estados\"].is_stop = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Functions**\n",
    "\n",
    "**Cleaning Dataset**\n",
    "\n",
    "Create a function to cleaning diferents expresion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multiple_replace(text):\n",
    "    dic = {r\"cia a\" : \"ciaa\",\"cia m\" : \"ciam\",\"sis a\" : \"sisa\", \"sis m\" : \"sism\",\n",
    "           'á' : 'a', 'é' : 'e', 'í' : 'i', 'ó' : 'o', 'ú' : 'u' }\n",
    "    # Create a regular expression  from the dictionary keys\n",
    "    regex = re.compile(\"(%s)\" % \"|\".join(map(re.escape, dic.keys())))\n",
    "    # For each match, look-up corresponding value in dictionary\n",
    "    return regex.sub(lambda mo: dic[mo.string[mo.start():mo.end()]], text.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    # Create a regular expression  from the dictionary keys\n",
    "    if isinstance(text, str):\n",
    "        dic = { r'[^\\w.]' : ' ', '[ 0-9 ]' : ''}\n",
    "        regex = re.compile(r'(%s)' % \"|\".join(dic.keys()))\n",
    "        lst = regex.sub(lambda mo: dic[[ k for k in dic if re.search(k, mo.string[mo.start():mo.end()])][0]], text).lower()\n",
    "    else:\n",
    "        lst = ' '\n",
    "    return [el for el in multiple_replace(lst).split('.') if re.search(\"[a-z]\", el)]# split texts into individual sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tokenize and lemmatize**\n",
    "\n",
    "Definition the function to tokenize and lemmatize (tokenizar y lematizar) the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_and_lemm_spacy(text):\n",
    "    doc = nlp(text)\n",
    "    lemmas = []\n",
    "    for token in doc:\n",
    "        if token.is_stop== False: # aprovechamos para eliminar ya las stopwords\n",
    "            if token.is_alpha== True: # Nos quedamos solo con los tokens que contienen letras \n",
    "                if token.pos_ not in ['CONJ', 'ADP', 'DET']: # eliminamos nombres propios, conjunciones, determinantes\n",
    "                    lemmas.append(token.lemma_.lower())\n",
    "    return lemmas\n",
    "\n",
    "def tokenize_only_spacy(text):\n",
    "    doc = nlp(text)\n",
    "    tokens = []\n",
    "    for token in doc:\n",
    "        if token.is_stop== False :\n",
    "            if token.is_alpha== True:\n",
    "                if token.pos_ not in ['CONJ', 'ADP', 'DET']:\n",
    "                    tokens.append(token.text.lower())\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_and_lemm_spacy_lst(lst):\n",
    "    return [tokenize_and_lemm_spacy(el) for el in lst]\n",
    "\n",
    "def tokenize_only_spacy_lst(lst):\n",
    "    return [tokenize_only_spacy(el) for el in lst]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "**Import Dataset** CAMBIAR A LEER HDF5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_hdf('../../data/corpus_train.h5', 'corpus')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "**Cleaning Dataset**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cleaning the columns \"CONCLUSION\" and \"REASONFORSTUDY\" like his name with \"*\\_CLEAN\" for not modificate the original column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 43min 38s, sys: 2.96 s, total: 43min 41s\n",
      "Wall time: 44min 13s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "\n",
    "df_train['document_sentences'] = df_train['CONCLUSION'].apply(clean_text)\n",
    "\n",
    "df_train['tokenized_sentences'] = df_train['document_sentences'].apply(tokenize_and_lemm_spacy_lst)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "**TRAIN dataset**\n",
    "\n",
    "TRAIN: Create a new dataset with all data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(26969, 8)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "**Export TRAIN**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_train.to_hdf('../../data/data.h5', key='TRAIN', mode='w')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "## Mediconnect Dataset\n",
    "\n",
    "**TEST dataset**\n",
    "\n",
    "TEST: Create a new dataset with mediconet data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = pd.read_hdf('../../data/corpus_test.h5', 'corpus')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "**Cleaning Dataset**\n",
    "\n",
    "Cleaning the columns \"CONCLUSION\" like his name with \"*\\_CLEAN\" for not modificate the original column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5min 1s, sys: 472 ms, total: 5min 2s\n",
      "Wall time: 5min 3s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "\n",
    "df_test['document_sentences'] = df_test['CONCLUSION'].apply(clean_text)\n",
    "\n",
    "df_test['tokenized_sentences'] = df_test['document_sentences'].apply(tokenize_and_lemm_spacy_lst)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "**Export TEST**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/NLP/lib/python3.7/site-packages/pandas/core/generic.py:2530: PerformanceWarning: \n",
      "your performance may suffer as PyTables will pickle object types that it cannot\n",
      "map directly to c-types [inferred_type->mixed,key->block1_values] [items->['CONCLUSION', 'document_sentences', 'tokenized_sentences']]\n",
      "\n",
      "  pytables.to_hdf(path_or_buf, key, self, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "df_test.to_hdf('../../data/test.h5', key='TEST', mode='w')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_nlp)",
   "language": "python",
   "name": "conda_nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
